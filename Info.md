# Анатомия кода: Как мы превратили сырые данные в инсайты о клиентах банка

## Введение в техническую архитектуру проекта

Когда мы начинаем аналитический проект, код становится нашим основным инструментом для преобразования данных в знания. В этом обзоре я покажу, как каждый блок кодаcontributing к финальному результату — пониманию причин оттока клиентов банка.

## Фундамент: Инициализация и настройка окружения

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
```

Эти импорты — наш базовый toolkit. Pandas работает с табличными данными, NumPy обеспечивает математическую основу, matplotlib и seaborn создают визуализации, а LabelEncoder из scikit-learn преобразует категориальные данные. Мы настраиваем стиль графиков для лучшей читаемости — это кажется мелочью, но значительно улучшает восприятие результатов.

## Первый контакт: Загрузка и диагностика данных

```python
df = pd.read_csv('Churn_Modelling.csv')
print(f"Размер датасета: {df.shape}")
df.head()
df.info()
```

Метод `read_csv()` загружает данные в DataFrame — основную структуру Pandas для работы с таблицами. `shape` показывает размерность данных, а `head()` выводит первые строки для беглого ознакомления. `info()` — это наш диагностический инструмент, показывающий типы данных и наличие пропусков.

## Тактический разведчик: Анализ распределений

```python
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Exited')
plt.title('Распределение клиентов по факту оттока')
```

Здесь мы используем seaborn's `countplot()` для визуализации распределения целевой переменной. Фигура размером 10x6 дюймов обеспечивает оптимальный баланс между детализацией и компактностью. Этот простой график сразу показывает нам дисбаланс классов — ключевую проблему в задачах прогнозирования оттока.

## Детектив аномалий: Анализ выбросов и паттернов

```python
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
numerical_features = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']

for i, feature in enumerate(numerical_features):
    row, col = i // 2, i % 2
    sns.boxplot(data=df, x='Exited', y=feature, ax=axes[row, col])
```

Этот код создает сетку графиков 2x2 для одновременного анализа нескольких признаков. `boxplot()` отображает медиану, квартили и выбросы — идеальный инструмент для сравнения распределений между группами. Мы видим, как возраст и баланс значительно различаются между ушедшими и оставшимися клиентами.

## Картограф взаимосвязей: Корреляционный анализ

```python
correlation_matrix = df[numeric_columns].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
```

Корреляционная матрица — это наша карта взаимосвязей. Метод `corr()` вычисляет коэффициенты корреляции Пирсона, а `heatmap()` визуализирует их в интуитивно понятном формате. Параметр `annot=True` добавляет числовые значения, а `cmap='coolwarm'` создает цветовую схему, где синий表示 отрицательные корреляции, красный — положительные.

## Инженер признаков: Создание новых переменных

```python
df_encoded['AgeGroup'] = pd.cut(df_encoded['Age'], 
                               bins=[0, 30, 45, 60, 100],
                               labels=['Young', 'Middle', 'Senior', 'Elderly'])

df_encoded['HighBalance'] = (df_encoded['Balance'] > df_encoded['Balance'].median()).astype(int)
```

Функция `pd.cut()` преобразует непрерывный возраст в категориальные группы — это помогает моделям лучше улавливать нелинейные зависимости. Создание бинарного признака `HighBalance` через сравнение с медианой — пример feature engineering, который может значительно улучшить предсказательную силу моделей.

## Переводчик категорий: Кодирование переменных

```python
df_encoded = pd.get_dummies(df_processed, columns=['Geography'], drop_first=True)
le = LabelEncoder()
df_encoded['Gender'] = le.fit_transform(df_encoded['Gender'])
```

Машинное обучение понимает только числа. `pd.get_dummies()` преобразует категориальную переменную "Geography" в набор бинарных признаков через one-hot encoding. `LabelEncoder` преобразует текстовые значения пола в числовые. Параметр `drop_first=True` избегает мультиколлинеарности.

## Шеридан данных: Отбор наиболее значимых признаков

```python
final_correlation = df_final.corr()['Exited'].sort_values(ascending=False)
top_features = final_correlation.drop('Exited').head(15)
```

После создания всех признаков мы должны выбрать наиболее информативные. Сортируя корреляции с целевой переменной, мы идентифицируем топ-15 наиболее прогнозных признаков. Это не только улучшает качество моделей, но и снижает вычислительную сложность.

## Архивариус: Сохранение результатов

```python
df_selected.to_csv('bank_churn_processed.csv', index=False)
```

Метод `to_csv()` сохраняет обработанные данные для последующего использования в моделях машинного обучения. Параметр `index=False` предотвращает сохранение индексов DataFrame как отдельного столбца — частая ошибка начинающих аналитиков.

## Заключение: Искусство и наука data analysis

Каждая строка кода в этом проекте служила конкретной цели — от очистки данных до генерации инсайтов. Красота анализа данных заключается в том, как отдельные технические операции объединяются в целостную картину, превращая сырые данные в стратегические insights.

Этот код — не просто набор команд, а рассказ о данных, где каждая функция играет свою роль в раскрытии истории, скрытой в числах.
